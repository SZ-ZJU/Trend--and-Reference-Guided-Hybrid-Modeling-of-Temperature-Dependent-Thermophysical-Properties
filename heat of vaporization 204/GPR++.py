import pandas as pd
import numpy as np
from sklearn.linear_model import HuberRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.model_selection import cross_val_score

# ==== å¸¸æ•°ä¸è·¯å¾„ ====
HV0, HVB, Tb0 = 9612.7, 15419.9, 222.543
T_ref = 298.15

# ==== è¯»å–æ•°æ® ====
df_main = pd.read_excel("heat of vaporization 204.xlsx", sheet_name="Sheet1")
Nk_all = df_main.iloc[:, 13:32].apply(pd.to_numeric, errors='coerce')  # 19åŸºå›¢
poly = PolynomialFeatures(degree=2, include_bias=False)
Nk_poly = poly.fit_transform(Nk_all)

# ==== è¯»å–éœ€è¦çš„åˆ— ====
temp_cols = list(df_main.columns[32:42])  # 10ä¸ªæ¸©åº¦åˆ—
hvap_cols = list(df_main.columns[42:52])  # 10ä¸ªæ±½åŒ–ç„“åˆ—

# æ•°å€¼åŒ–å¤„ç†
for col in temp_cols + hvap_cols:
    df_main[col] = pd.to_numeric(df_main[col], errors='coerce')

# ==== Tb æ¨¡å‹ (å‚è€ƒæ¸©åº¦é¢„æµ‹æ¨¡å‹) ====
Tb_raw = df_main.iloc[:, 5].values  # è¯»å–å‚è€ƒæ¸©åº¦åˆ—
mask_tb_ref = ~np.isnan(Tb_raw)  # ç­›é€‰å‚è€ƒæ¸©åº¦æœ‰æ•ˆæ•°æ®
model_Tb = HuberRegressor(max_iter=10000).fit(Nk_poly[mask_tb_ref], np.exp(Tb_raw[mask_tb_ref] / Tb0))
Tb_pred_all = Tb0 * np.log(np.clip(model_Tb.predict(Nk_poly), 1e-6, None))  # æ‰€æœ‰ç‰©è´¨çš„å‚è€ƒæ¸©åº¦é¢„æµ‹

# ==== HVPb æ¨¡å‹ (æ±½åŒ–ç„“é¢„æµ‹æ¨¡å‹) ====
df_Tb = pd.read_excel("selected_25_descriptors_data_boiling_point.xlsx")
X_Tb = df_Tb.drop(columns=["Heat of vaporization at boiling temperature"])
rf_Tb = RandomForestRegressor(random_state=42).fit(X_Tb, df_Tb["Heat of vaporization at boiling temperature"])
HVap_Tb_all = rf_Tb.predict(X_Tb)  # é¢„æµ‹å‚è€ƒæ±½åŒ–ç„“

# ==== A_k ç³»æ•°è®­ç»ƒ ====
G = Nk_all.values  # (n, 19) åŸºå›¢æ•°æ®
X_rows, y_rows = [], []
temp_eval = []  # ä¿å­˜æ¸©åº¦ç‚¹ä¿¡æ¯ç”¨äºè¯„ä¼°

# ä¿®æ­£ï¼šæ­£ç¡®æ„å»ºè®­ç»ƒé›†ï¼Œå¤„ç†NaNå€¼
for i in range(len(df_main)):  # éå†æ‰€æœ‰ç‰©è´¨
    for j, (tcol, hvcol) in enumerate(zip(temp_cols, hvap_cols)):
        Tj = df_main.at[i, tcol]  # æ¸©åº¦å€¼
        Hvapj = df_main.at[i, hvcol]  # æ±½åŒ–ç„“å€¼

        # è·³è¿‡NaNå€¼
        if np.isnan(Tj) or np.isnan(Hvapj):
            continue

        Tb_i = Tb_pred_all[i]  # ç‰©è´¨içš„å‚è€ƒæ¸©åº¦
        HVap_Tb_i = HVap_Tb_all[i]  # ç‰©è´¨içš„å‚è€ƒæ±½åŒ–ç„“

        # ç‰¹å¾ï¼š(T - T_ref) Ã— G
        Xj = (Tj - Tb_i) * G[i]  # å½¢çŠ¶: (19,)

        # ç›®æ ‡ï¼šHvap - Hvap_ref
        yj = Hvapj - HVap_Tb_i

        X_rows.append(Xj)
        y_rows.append(yj)
        temp_eval.append((tcol, hvcol, i, j))

X_A = np.array(X_rows)  # (n_samples, 19)
y_A = np.array(y_rows)  # (n_samples,)

# è®­ç»ƒ A_k ç³»æ•°æ¨¡å‹
A_solver = HuberRegressor(fit_intercept=False, max_iter=5000)
A_solver.fit(X_A, y_A)
A_vec = A_solver.coef_  # é•¿åº¦19ï¼Œå¯¹åº”åŸºå›¢åˆ—é¡ºåº

# ==== ç”ŸæˆåŸºå‡†æ±½åŒ–ç„“é¢„æµ‹ ====
HVap_pred_baseline = pd.DataFrame(index=df_main.index, columns=hvap_cols, dtype=float)

for i in range(len(df_main)):  # éå†æ‰€æœ‰ç‰©è´¨
    Tb_i = Tb_pred_all[i]  # ç‰©è´¨içš„å‚è€ƒæ¸©åº¦
    HVap_Tb_i = HVap_Tb_all[i]  # ç‰©è´¨içš„å‚è€ƒæ±½åŒ–ç„“

    for j, (tcol, hvcol) in enumerate(zip(temp_cols, hvap_cols)):
        Tj = df_main.at[i, tcol]  # æ¸©åº¦å€¼

        if np.isnan(Tj):
            HVap_pred_baseline.at[i, hvcol] = np.nan
            continue

        # ç‰¹å¾ï¼š(T - T_ref) Ã— G
        Xj = (Tj - Tb_i) * G[i]

        # é¢„æµ‹ï¼šHvap_ref + A_k Ã— (T - T_ref) Ã— G
        HVap_pred_j = HVap_Tb_i + Xj @ A_vec
        HVap_pred_baseline.at[i, hvcol] = HVap_pred_j

# ==== æ®‹å·®æœºå™¨å­¦ä¹ æ¨¡å‹ ====
print("è®­ç»ƒæ®‹å·®æœºå™¨å­¦ä¹ æ¨¡å‹...")

# æ„å»ºæ®‹å·®è®­ç»ƒæ•°æ®é›†
residual_features = []
residual_targets = []
sample_info = []  # ä¿å­˜æ ·æœ¬ä¿¡æ¯ç”¨äºè¿½è¸ª

for tcol, hvcol in zip(temp_cols, hvap_cols):
    Tj = df_main[tcol].to_numpy()
    Hvapj = df_main[hvcol].to_numpy()
    msk = (~np.isnan(Tj)) & (~np.isnan(Hvapj))

    for i in np.where(msk)[0]:
        # åŸºç¡€ç‰¹å¾ï¼šåŸºå›¢ç»„æˆ
        base_features = list(G[i])

        # æ¸©åº¦ç›¸å…³ç‰¹å¾
        temp_features = [
            Tj[i],  # ç»å¯¹æ¸©åº¦
            Tj[i] - Tb_pred_all[i],  # ç›¸å¯¹äºå‚è€ƒæ¸©åº¦çš„å·®å€¼
            Tj[i] / Tb_pred_all[i] if Tb_pred_all[i] > 0 else 0,  # ç›¸å¯¹æ¸©åº¦
            np.log(Tj[i]) if Tj[i] > 0 else 0,  # æ¸©åº¦å¯¹æ•°
        ]

        # åŸºå‡†é¢„æµ‹å€¼ä½œä¸ºç‰¹å¾
        baseline_pred = HVap_pred_baseline.at[i, hvcol]
        baseline_features = [baseline_pred]

        # å‚è€ƒå€¼ç‰¹å¾
        ref_features = [
            Tb_pred_all[i],  # å‚è€ƒæ¸©åº¦
            HVap_Tb_all[i],  # å‚è€ƒæ±½åŒ–ç„“
        ]

        # ç»„åˆæ‰€æœ‰ç‰¹å¾
        all_features = base_features + temp_features + baseline_features + ref_features
        residual_features.append(all_features)

        # æ®‹å·®ç›®æ ‡ï¼šå®é™…å€¼ - åŸºå‡†é¢„æµ‹å€¼
        residual = Hvapj[i] - baseline_pred
        residual_targets.append(residual)

        sample_info.append((i, tcol, hvcol))

residual_features = np.array(residual_features)
residual_targets = np.array(residual_targets)

print(f"æ®‹å·®è®­ç»ƒé›†å½¢çŠ¶: {residual_features.shape}")
print(f"æ®‹å·®ç›®æ ‡å½¢çŠ¶: {residual_targets.shape}")

# æ ‡å‡†åŒ–ç‰¹å¾
scaler = StandardScaler()
residual_features_scaled = scaler.fit_transform(residual_features)

# è®­ç»ƒæ®‹å·®æ¨¡å‹ï¼ˆä½¿ç”¨æ¢¯åº¦æå‡å›å½’ï¼‰
residual_model = GradientBoostingRegressor(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42
)

# äº¤å‰éªŒè¯è¯„ä¼°æ®‹å·®æ¨¡å‹
cv_scores = cross_val_score(residual_model, residual_features_scaled, residual_targets,
                            cv=5, scoring='r2')
print(f"æ®‹å·®æ¨¡å‹äº¤å‰éªŒè¯ RÂ²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# è®­ç»ƒæœ€ç»ˆæ®‹å·®æ¨¡å‹
residual_model.fit(residual_features_scaled, residual_targets)

# ==== ç”Ÿæˆæœ€ç»ˆé¢„æµ‹ï¼ˆåŸºå‡† + æ®‹å·®ä¿®æ­£ï¼‰ ====
HVap_pred_final = pd.DataFrame(index=df_main.index, columns=hvap_cols, dtype=float)

for tcol, hvcol in zip(temp_cols, hvap_cols):
    Tj = df_main[tcol].to_numpy()

    # ä¸ºæ‰€æœ‰æ ·æœ¬æ„å»ºç‰¹å¾
    features_list = []
    valid_indices = []

    for i in range(len(df_main)):
        if np.isnan(Tj[i]):
            continue

        # æ„å»ºä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç‰¹å¾
        base_features = list(G[i])
        temp_features = [
            Tj[i],
            Tj[i] - Tb_pred_all[i],
            Tj[i] / Tb_pred_all[i] if Tb_pred_all[i] > 0 else 0,
            np.log(Tj[i]) if Tj[i] > 0 else 0,
        ]
        baseline_pred = HVap_pred_baseline.at[i, hvcol]
        baseline_features = [baseline_pred]
        ref_features = [Tb_pred_all[i], HVap_Tb_all[i]]

        all_features = base_features + temp_features + baseline_features + ref_features
        features_list.append(all_features)
        valid_indices.append(i)

    if features_list:
        features_array = np.array(features_list)
        features_scaled = scaler.transform(features_array)

        # é¢„æµ‹æ®‹å·®
        residual_pred = residual_model.predict(features_scaled)

        # æœ€ç»ˆé¢„æµ‹ = åŸºå‡†é¢„æµ‹ + æ®‹å·®ä¿®æ­£
        for idx, residual_val in zip(valid_indices, residual_pred):
            final_pred = HVap_pred_baseline.at[idx, hvcol] + residual_val
            HVap_pred_final.at[idx, hvcol] = final_pred

    # å¯¹äºæ— æ•ˆæ¸©åº¦ç‚¹ï¼Œä¿æŒNaN
    HVap_pred_final[hvcol] = np.where(np.isnan(Tj), np.nan, HVap_pred_final[hvcol])

# ==== è¯„ä¼°æ¨¡å‹æ€§èƒ½ ====
# åŸºå‡†æ¨¡å‹è¯„ä¼°
print("\n=== åŸºå‡†æ¨¡å‹æ€§èƒ½ ===")
y_true_all, y_pred_baseline = [], []
for hvcol in hvap_cols:
    m = (~df_main[hvcol].isna()) & (~HVap_pred_baseline[hvcol].isna())
    if m.any():
        y_true_all.append(df_main.loc[m, hvcol].to_numpy())
        y_pred_baseline.append(HVap_pred_baseline.loc[m, hvcol].to_numpy())

y_true_all = np.concatenate(y_true_all)
y_pred_baseline = np.concatenate(y_pred_baseline)

mse_baseline = mean_squared_error(y_true_all, y_pred_baseline)
r2_baseline = r2_score(y_true_all, y_pred_baseline)
print(f"åŸºå‡†æ¨¡å‹ - MSE: {mse_baseline:.6f}, RÂ²: {r2_baseline:.6f}")

# æœ€ç»ˆæ¨¡å‹è¯„ä¼°
print("\n=== æœ€ç»ˆæ¨¡å‹æ€§èƒ½ï¼ˆåŸºå‡† + æ®‹å·®ä¿®æ­£ï¼‰===")
y_true_all, y_pred_final = [], []
for hvcol in hvap_cols:
    m = (~df_main[hvcol].isna()) & (~HVap_pred_final[hvcol].isna())
    if m.any():
        y_true_all.append(df_main.loc[m, hvcol].to_numpy())
        y_pred_final.append(HVap_pred_final.loc[m, hvcol].to_numpy())

y_true_all = np.concatenate(y_true_all)
y_pred_final = np.concatenate(y_pred_final)

mse_final = mean_squared_error(y_true_all, y_pred_final)
r2_final = r2_score(y_true_all, y_pred_final)
print(f"æœ€ç»ˆæ¨¡å‹ - MSE: {mse_final:.6f}, RÂ²: {r2_final:.6f}")

# æ”¹è¿›ç¨‹åº¦
improvement = r2_final - r2_baseline
print(f"\næ”¹è¿›ç¨‹åº¦: RÂ² æå‡äº† {improvement:.4f} ({improvement / r2_baseline * 100:.2f}%)")

# ==== åˆ†æ¸©åº¦ç‚¹è¯„ä¼° ====
print("\nåˆ†æ¸©åº¦ç‚¹è¯„ä¼°:")
for tcol, hvcol in zip(temp_cols, hvap_cols):
    m = (~df_main[tcol].isna()) & (~df_main[hvcol].isna()) & (~HVap_pred_final[hvcol].isna())
    if m.any():
        hvap_true = df_main.loc[m, hvcol].to_numpy()
        hvap_pred = HVap_pred_final.loc[m, hvcol].to_numpy()
        mse_temp = mean_squared_error(hvap_true, hvap_pred)
        r2_temp = r2_score(hvap_true, hvap_pred)
        print(f"  {tcol}: MSE = {mse_temp:.6f}, R2 = {r2_temp:.6f}")

# ==== ä¿å­˜æœ€ç»ˆçš„ç»“æœ ====
id_col = df_main.columns[0]  # ç‰©è´¨ID/åç§°æ‰€åœ¨åˆ—
out_path = "hvap_actual_vs_pred_with_residual_correction.xlsx"

rows = []
for idx, _ in df_main.iterrows():
    ID = df_main.at[idx, id_col]
    for j, (tcol, hvcol) in enumerate(zip(temp_cols, hvap_cols), start=1):
        T = df_main.at[idx, tcol]
        HVap_act = df_main.at[idx, hvcol]
        HVap_base = HVap_pred_baseline.at[idx, hvcol] if pd.notna(HVap_pred_baseline.at[idx, hvcol]) else np.nan
        HVap_final = HVap_pred_final.at[idx, hvcol] if pd.notna(HVap_pred_final.at[idx, hvcol]) else np.nan

        # è®¡ç®—è¯¯å·®
        err_base = (HVap_base - HVap_act) if (pd.notna(HVap_base) and pd.notna(HVap_act)) else np.nan
        err_final = (HVap_final - HVap_act) if (pd.notna(HVap_final) and pd.notna(HVap_act)) else np.nan
        residual_correction = (HVap_final - HVap_base) if (pd.notna(HVap_final) and pd.notna(HVap_base)) else np.nan

        rows.append({
            id_col: ID,
            "temp_index": j,
            "temp_col": tcol,
            "T": T,
            "HVap_actual": HVap_act,
            "HVap_baseline": HVap_base,
            "HVap_final": HVap_final,
            "error_baseline": err_base,
            "error_final": err_final,
            "residual_correction": residual_correction,
            "T_ref": Tb_pred_all[idx],
            "HVap_ref": HVap_Tb_all[idx]
        })

long_compare = pd.DataFrame(rows).sort_values([id_col, "temp_index"])

with pd.ExcelWriter(out_path, engine="xlsxwriter") as writer:
    long_compare.to_excel(writer, sheet_name="compare_long", index=False)

print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {out_path}")
# â€”â€” ç®€æ´ç›¸å¯¹è¯¯å·®ç»Ÿè®¡ï¼ˆæœ€ç»ˆï¼‰â€”â€”
relative_error_final = np.abs((y_pred_final - y_true_all) / y_true_all) * 100
within_1pct_final  = np.sum(relative_error_final <= 1)
within_5pct_final  = np.sum(relative_error_final <= 5)
within_10pct_final = np.sum(relative_error_final <= 10)
ard_final = np.mean(relative_error_final)

print("\nğŸ“Š æ€»æ¨¡å‹è¯„ä¼°ï¼ˆåŸºå‡† + æ®‹å·®ä¿®æ­£ï¼‰ï¼š")
print(f"RÂ²  = {r2_final:.4f}")
print(f"MSE = {mse_final:.6f}")
print(f"ARD = {ard_final:.2f}%")
print(f"âœ… è¯¯å·® â‰¤ 1% çš„æ•°æ®ç‚¹æ•°é‡: {within_1pct_final}")
print(f"âœ… è¯¯å·® â‰¤ 5% çš„æ•°æ®ç‚¹æ•°é‡: {within_5pct_final}")
print(f"âœ… è¯¯å·® â‰¤ 10% çš„æ•°æ®ç‚¹æ•°é‡: {within_10pct_final}")
