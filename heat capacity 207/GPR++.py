import pandas as pd
import numpy as np
from sklearn.linear_model import HuberRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score

# ========= 1. è¯»å–æ•°æ® =========
file_path = "heat capacity 207.xlsx"
sheet = "Sheet1"
df = pd.read_excel(file_path, sheet_name=sheet).copy()

df = df.dropna(subset=[df.columns[0]])
df[df.columns[0]] = df[df.columns[0]].astype(int)

# ========= 2. åˆ—å®šä¹‰ =========
group_cols = list(df.columns[11:30])  # 19ä¸ªåŸºå›¢åˆ—
temp_cols = list(df.columns[30:40])  # 10ä¸ªæ¸©åº¦ç‚¹
cp_cols = list(df.columns[40:50])  # 10ä¸ª Cp å€¼
target_column_T1 = 'ASPEN Half Critical T'

# åšä¸€ä¸ªæ¸©åº¦åˆ— -> å¯¹åº”Cpåˆ— çš„æ˜ å°„ï¼ˆç´¢å¼•ä¸€ä¸€å¯¹åº”ï¼‰
temp_to_cp = {t: c for t, c in zip(temp_cols, cp_cols)}

# æ•°å€¼åŒ–
for cols in [group_cols, temp_cols, cp_cols]:
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors='coerce')

# ========= 3. å­æ¨¡å‹è®­ç»ƒï¼šT_ref(=T1) ä¸ C_pref(=Cp1) =========
X_groups = df[group_cols].fillna(0)

valid_mask = ~df[target_column_T1].isna()
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly_train = poly.fit_transform(X_groups[valid_mask])

y_T1 = df.loc[valid_mask, target_column_T1].to_numpy()
T1_model = GradientBoostingRegressor(
    n_estimators=300, learning_rate=0.05, max_depth=4, random_state=0
).fit(X_poly_train, y_T1)

# å¯¹æ‰€æœ‰æ ·æœ¬é¢„æµ‹ T_ref
X_poly_all = poly.transform(X_groups)
T_ref_pred = T1_model.predict(X_poly_all)  # (n,)

# Cp1æ¨¡å‹ï¼ˆç¤ºä¾‹é‡Œç”¨ç¬¬9åˆ—ä½œä¸ºç›®æ ‡ï¼‰
y_cp1_target = df.iloc[:, 9].to_numpy()
Cp1_model = HuberRegressor(max_iter=9000).fit(X_groups, y_cp1_target)

# å¯¹æ‰€æœ‰æ ·æœ¬é¢„æµ‹ C_pref
C_pref_pred = Cp1_model.predict(X_groups)  # (n,)

# ========= 4. æ„é€  A_k çš„è®­ç»ƒé›†ï¼ˆç‰©è´¨Ã—æ¸©åº¦ç‚¹å±•å¼€ï¼‰=========
G = X_groups.to_numpy()  # (n, 19)
X_rows, y_rows = [], []
temp_eval = []  # ä¿å­˜ (tcol, cpcol, msk) ä»¥ä¾¿åˆ†æ¸©åº¦è¯„ä¼°

for tcol, cpcol in zip(temp_cols, cp_cols):
    Tj = df[tcol].to_numpy()  # (n,)
    CPj = df[cpcol].to_numpy()  # (n,)
    msk = (~np.isnan(Tj)) & (~np.isnan(CPj))
    if msk.sum() == 0:
        continue

    # ç‰¹å¾ï¼š(T - T_ref)[:, None] * G  â†’ (n_j, 19)
    Xj = ((Tj - T_ref_pred)[:, None] * G)[msk]
    # ç›®æ ‡ï¼šCp - C_prefï¼ˆç”¨äºè®­ç»ƒAï¼‰
    yj = (CPj - C_pref_pred)[msk]

    X_rows.append(Xj)
    y_rows.append(yj)
    temp_eval.append((tcol, cpcol, msk))

X_A = np.vstack(X_rows)  # (sum_j n_j, 19)
y_A = np.concatenate(y_rows)  # (sum_j n_j,)

# ========= 5. æ‹Ÿåˆ A_kï¼ˆæ— æˆªè·ï¼›æˆªè·ç”± C_pref æ‰¿æ‹…ï¼‰=========
A_solver = HuberRegressor(fit_intercept=False, max_iter=5000)
A_solver.fit(X_A, y_A)
A_vec = A_solver.coef_  # é•¿åº¦19ï¼Œå¯¹åº” group_cols é¡ºåº

# ========= 6. ç”ŸæˆåŸºå‡†é¢„æµ‹ =========
Cp_pred_baseline = pd.DataFrame(index=df.index, columns=cp_cols, dtype=float)
for tcol, cpcol in zip(temp_cols, cp_cols):
    Tj = df[tcol].to_numpy()
    Xj = (Tj - T_ref_pred)[:, None] * G
    Cp_pred_j = C_pref_pred + Xj @ A_vec
    Cp_pred_baseline[cpcol] = np.where(np.isnan(Tj), np.nan, Cp_pred_j)

# ========= 7. æ®‹å·®æœºå™¨å­¦ä¹ æ¨¡å‹ =========
print("è®­ç»ƒæ®‹å·®æœºå™¨å­¦ä¹ æ¨¡å‹...")

# æ„å»ºæ®‹å·®è®­ç»ƒæ•°æ®é›†
residual_features = []
residual_targets = []
sample_info = []  # ä¿å­˜æ ·æœ¬ä¿¡æ¯ç”¨äºè¿½è¸ª

for tcol, cpcol in zip(temp_cols, cp_cols):
    Tj = df[tcol].to_numpy()
    CPj = df[cpcol].to_numpy()
    msk = (~np.isnan(Tj)) & (~np.isnan(CPj))

    for i in np.where(msk)[0]:
        # åŸºç¡€ç‰¹å¾ï¼šåŸºå›¢ç»„æˆ
        base_features = list(G[i])

        # æ¸©åº¦ç›¸å…³ç‰¹å¾
        temp_features = [
            Tj[i],  # ç»å¯¹æ¸©åº¦
            Tj[i] - T_ref_pred[i],  # ç›¸å¯¹äºå‚è€ƒæ¸©åº¦çš„å·®å€¼
            Tj[i] / T_ref_pred[i] if T_ref_pred[i] > 0 else 0,  # ç›¸å¯¹æ¸©åº¦
            np.log(Tj[i]) if Tj[i] > 0 else 0,  # æ¸©åº¦å¯¹æ•°
        ]

        # åŸºå‡†é¢„æµ‹å€¼ä½œä¸ºç‰¹å¾
        baseline_pred = Cp_pred_baseline.at[i, cpcol]
        baseline_features = [baseline_pred]

        # ç»„åˆæ‰€æœ‰ç‰¹å¾
        all_features = base_features + temp_features + baseline_features
        residual_features.append(all_features)

        # æ®‹å·®ç›®æ ‡ï¼šå®é™…å€¼ - åŸºå‡†é¢„æµ‹å€¼
        residual = CPj[i] - baseline_pred
        residual_targets.append(residual)

        sample_info.append((i, tcol, cpcol))

residual_features = np.array(residual_features)
residual_targets = np.array(residual_targets)

print(f"æ®‹å·®è®­ç»ƒé›†å½¢çŠ¶: {residual_features.shape}")
print(f"æ®‹å·®ç›®æ ‡å½¢çŠ¶: {residual_targets.shape}")

# æ ‡å‡†åŒ–ç‰¹å¾
scaler = StandardScaler()
residual_features_scaled = scaler.fit_transform(residual_features)

# è®­ç»ƒæ®‹å·®æ¨¡å‹ï¼ˆä½¿ç”¨æ¢¯åº¦æå‡å›å½’ï¼‰
residual_model = GradientBoostingRegressor(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42
)

# äº¤å‰éªŒè¯è¯„ä¼°æ®‹å·®æ¨¡å‹
cv_scores = cross_val_score(residual_model, residual_features_scaled, residual_targets,
                            cv=5, scoring='r2')
print(f"æ®‹å·®æ¨¡å‹äº¤å‰éªŒè¯ RÂ²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# è®­ç»ƒæœ€ç»ˆæ®‹å·®æ¨¡å‹
residual_model.fit(residual_features_scaled, residual_targets)

# ========= 8. ç”Ÿæˆæœ€ç»ˆé¢„æµ‹ï¼ˆåŸºå‡† + æ®‹å·®ä¿®æ­£ï¼‰=========
Cp_pred_final = pd.DataFrame(index=df.index, columns=cp_cols, dtype=float)

for tcol, cpcol in zip(temp_cols, cp_cols):
    Tj = df[tcol].to_numpy()

    # ä¸ºæ‰€æœ‰æ ·æœ¬æ„å»ºç‰¹å¾
    features_list = []
    valid_indices = []

    for i in range(len(df)):
        if np.isnan(Tj[i]):
            continue

        # æ„å»ºä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç‰¹å¾
        base_features = list(G[i])
        temp_features = [
            Tj[i],
            Tj[i] - T_ref_pred[i],
            Tj[i] / T_ref_pred[i] if T_ref_pred[i] > 0 else 0,
            np.log(Tj[i]) if Tj[i] > 0 else 0,
        ]
        baseline_pred = Cp_pred_baseline.at[i, cpcol]
        baseline_features = [baseline_pred]

        all_features = base_features + temp_features + baseline_features
        features_list.append(all_features)
        valid_indices.append(i)

    if features_list:
        features_array = np.array(features_list)
        features_scaled = scaler.transform(features_array)

        # é¢„æµ‹æ®‹å·®
        residual_pred = residual_model.predict(features_scaled)

        # æœ€ç»ˆé¢„æµ‹ = åŸºå‡†é¢„æµ‹ + æ®‹å·®ä¿®æ­£
        for idx, residual_val in zip(valid_indices, residual_pred):
            final_pred = Cp_pred_baseline.at[idx, cpcol] + residual_val
            Cp_pred_final.at[idx, cpcol] = final_pred

    # å¯¹äºæ— æ•ˆæ¸©åº¦ç‚¹ï¼Œä¿æŒNaN
    Cp_pred_final[cpcol] = np.where(np.isnan(Tj), np.nan, Cp_pred_final[cpcol])

# ========= 9. è¯„ä¼°æ¨¡å‹æ€§èƒ½ =========
# 9.1 åŸºå‡†æ¨¡å‹è¯„ä¼°
print("\n=== åŸºå‡†æ¨¡å‹æ€§èƒ½ ===")
y_true_all, y_pred_baseline = [], []
for cpcol in cp_cols:
    m = (~df[cpcol].isna()) & (~Cp_pred_baseline[cpcol].isna())
    if m.any():
        y_true_all.append(df.loc[m, cpcol].to_numpy())
        y_pred_baseline.append(Cp_pred_baseline.loc[m, cpcol].to_numpy())

y_true_all = np.concatenate(y_true_all)
y_pred_baseline = np.concatenate(y_pred_baseline)

mse_baseline = mean_squared_error(y_true_all, y_pred_baseline)
r2_baseline = r2_score(y_true_all, y_pred_baseline)
print(f"åŸºå‡†æ¨¡å‹ - MSE: {mse_baseline:.6f}, RÂ²: {r2_baseline:.6f}")

# 9.2 æœ€ç»ˆæ¨¡å‹è¯„ä¼°
print("\n=== æœ€ç»ˆæ¨¡å‹æ€§èƒ½ï¼ˆåŸºå‡† + æ®‹å·®ä¿®æ­£ï¼‰===")
y_true_all, y_pred_final = [], []
for cpcol in cp_cols:
    m = (~df[cpcol].isna()) & (~Cp_pred_final[cpcol].isna())
    if m.any():
        y_true_all.append(df.loc[m, cpcol].to_numpy())
        y_pred_final.append(Cp_pred_final.loc[m, cpcol].to_numpy())

y_true_all = np.concatenate(y_true_all)
y_pred_final = np.concatenate(y_pred_final)

mse_final = mean_squared_error(y_true_all, y_pred_final)
r2_final = r2_score(y_true_all, y_pred_final)
print(f"æœ€ç»ˆæ¨¡å‹ - MSE: {mse_final:.6f}, RÂ²: {r2_final:.6f}")

# 9.3 æ”¹è¿›ç¨‹åº¦
improvement = r2_final - r2_baseline
print(f"\næ”¹è¿›ç¨‹åº¦: RÂ² æå‡äº† {improvement:.4f} ({improvement / r2_baseline * 100:.2f}%)")

# 9.4 åˆ†æ¸©åº¦ç‚¹è¯„ä¼°æœ€ç»ˆæ¨¡å‹
print("\n=== åˆ†æ¸©åº¦ç‚¹è¯„ä¼°ï¼ˆæœ€ç»ˆæ¨¡å‹ï¼‰===")
for tcol, cpcol, msk in temp_eval:
    cp_true = df[cpcol].to_numpy()[msk]
    cp_pred = Cp_pred_final[cpcol].to_numpy()[msk]
    print(f"  {tcol}: MSE = {mean_squared_error(cp_true, cp_pred):.6f}, "
          f"RÂ² = {r2_score(cp_true, cp_pred):.6f}")

# ========= 10. ä¿å­˜ç»“æœ =========
id_col = df.columns[0]  # ç‰©è´¨ID/åç§°æ‰€åœ¨åˆ—
out_path = "cp_actual_vs_pred_with_residual_correction.xlsx"

rows = []
for idx, _ in df.iterrows():
    ID = df.at[idx, id_col]
    for j, (tcol, cpcol) in enumerate(zip(temp_cols, cp_cols), start=1):
        T = df.at[idx, tcol]
        Cp_act = df.at[idx, cpcol]
        Cp_base = Cp_pred_baseline.at[idx, cpcol] if pd.notna(Cp_pred_baseline.at[idx, cpcol]) else np.nan
        Cp_final = Cp_pred_final.at[idx, cpcol] if pd.notna(Cp_pred_final.at[idx, cpcol]) else np.nan

        # è®¡ç®—è¯¯å·®
        err_base = (Cp_base - Cp_act) if (pd.notna(Cp_base) and pd.notna(Cp_act)) else np.nan
        err_final = (Cp_final - Cp_act) if (pd.notna(Cp_final) and pd.notna(Cp_act)) else np.nan
        residual_correction = (Cp_final - Cp_base) if (pd.notna(Cp_final) and pd.notna(Cp_base)) else np.nan

        rows.append({
            id_col: ID,
            "temp_index": j,
            "temp_col": tcol,
            "T": T,
            "Cp_actual": Cp_act,
            "Cp_baseline": Cp_base,
            "Cp_final": Cp_final,
            "error_baseline": err_base,
            "error_final": err_final,
            "residual_correction": residual_correction,
            "T_ref": T_ref_pred[idx] if idx < len(T_ref_pred) else np.nan,
            "Cp_ref": C_pref_pred[idx] if idx < len(C_pref_pred) else np.nan
        })

long_compare = pd.DataFrame(rows).sort_values([id_col, "temp_index"])

with pd.ExcelWriter(out_path, engine="xlsxwriter") as writer:
    long_compare.to_excel(writer, sheet_name="compare_long", index=False)

print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {out_path}")
# ========= 9.x ç®€å•è¯¯å·®ç»Ÿè®¡ =========
relative_error = np.abs((y_pred_final - y_true_all) / y_true_all) * 100
within_1pct = np.sum(relative_error <= 1)
within_5pct = np.sum(relative_error <= 5)
within_10pct = np.sum(relative_error <= 10)

ard = np.mean(relative_error)  # å¹³å‡ç›¸å¯¹åå·® %

print("\nğŸ“Š æ€»æ¨¡å‹è¯„ä¼°ï¼ˆå« slopeÃ—T ç‰¹å¾ï¼‰ï¼š")
print(f"RÂ²  = {r2_final:.4f}")
print(f"MSE = {mse_final:.2f}")
print(f"ARD = {ard:.2f}%")
print(f"âœ… è¯¯å·® â‰¤ 1% çš„æ•°æ®ç‚¹æ•°é‡: {within_1pct}")
print(f"âœ… è¯¯å·® â‰¤ 5% çš„æ•°æ®ç‚¹æ•°é‡: {within_5pct}")
print(f"âœ… è¯¯å·® â‰¤ 10% çš„æ•°æ®ç‚¹æ•°é‡: {within_10pct}")
