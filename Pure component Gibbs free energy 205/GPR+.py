import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import HuberRegressor
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score

# ==== 1. æ•°æ®åŠ è½½ ====
df = pd.read_excel("Gibbs free energy 205.xlsx", sheet_name="Sheet6")

group_cols = df.columns[12:31]  # ç¬¬13~31åˆ—ï¼šåŸºå›¢
temp_cols = df.columns[31:41]  # ç¬¬32~41åˆ—ï¼šæ¸©åº¦
v_cols = df.columns[41:51]  # ç¬¬42~51åˆ—ï¼šç›®æ ‡å˜é‡ï¼ˆå‰å¸ƒæ–¯è‡ªç”±èƒ½ï¼‰

# ==== 2. æ•°æ®é¢„å¤„ç† ====
# ç¡®ä¿æ•°å€¼åˆ—æ­£ç¡®è½¬æ¢
for col in temp_cols.tolist() + v_cols.tolist():
    df[col] = pd.to_numeric(df[col], errors='coerce')

# åŸºå›¢æ•°æ®
Nk_all = df[group_cols].apply(pd.to_numeric, errors='coerce')

# ==== 3. Hvap æ¨¡å‹ï¼ˆTbï¼‰ ====
df_Tb = pd.read_excel("selected_25_descriptors_boiling.xlsx")
X_Tb = df_Tb.drop(columns=["ASPEN Vapor pressure at BoilingTemperature(bar)"])
y_Tb = df_Tb["ASPEN Vapor pressure at BoilingTemperature(bar)"]
rf_Tb = RandomForestRegressor(random_state=42).fit(X_Tb, y_Tb)
HVap_Tb_all = rf_Tb.predict(X_Tb)

# ==== 4. Tb æ¨¡å‹é¢„æµ‹ï¼ˆæ ‡å‡†åŒ– + å¤šé¡¹å¼ï¼‰====
Tb_raw = df.iloc[:, 5].values
Tb0 = 222.543

poly = PolynomialFeatures(degree=2, include_bias=False)
Nk_poly = poly.fit_transform(Nk_all)
scaler_tb = StandardScaler()
Nk_scaled = scaler_tb.fit_transform(Nk_poly)

mask_tb = ~np.isnan(Tb_raw)
model_Tb = HuberRegressor(max_iter=10000)
model_Tb.fit(Nk_scaled[mask_tb], np.exp(Tb_raw[mask_tb] / Tb0))
Tb_pred_all = Tb0 * np.log(np.clip(model_Tb.predict(Nk_scaled), 1e-6, None))

# ==== 5. A_k ç³»æ•°è®­ç»ƒ ====
# ä½¿ç”¨åŸå§‹19ä¸ªåŸºå›¢
G = Nk_all.values  # (n, 19) åŸå§‹åŸºå›¢æ•°æ®
X_rows, y_rows = [], []
temp_eval = []  # ä¿å­˜æ¸©åº¦ç‚¹ä¿¡æ¯ç”¨äºè¯„ä¼°

# æ„å»ºè®­ç»ƒé›†
for i in range(len(df)):  # éå†æ‰€æœ‰ç‰©è´¨
    for j, (tcol, vcol) in enumerate(zip(temp_cols, v_cols)):
        Tj = df.at[i, tcol]  # æ¸©åº¦å€¼
        Vj = df.at[i, vcol]  # å‰å¸ƒæ–¯è‡ªç”±èƒ½å€¼

        # è·³è¿‡NaNå€¼
        if np.isnan(Tj) or np.isnan(Vj):
            continue

        Tb_i = Tb_pred_all[i]  # ç‰©è´¨içš„å‚è€ƒæ¸©åº¦
        HVap_Tb_i = HVap_Tb_all[i]  # ç‰©è´¨içš„å‚è€ƒå‰å¸ƒæ–¯è‡ªç”±èƒ½å€¼

        # ç‰¹å¾ï¼š(T - T_ref) Ã— G (ä½¿ç”¨åŸå§‹19ä¸ªåŸºå›¢)
        Xj = (Tj - Tb_i) * G[i]  # å½¢çŠ¶: (19,)

        # ç›®æ ‡ï¼šV - V_ref
        yj = Vj - HVap_Tb_i

        X_rows.append(Xj)
        y_rows.append(yj)
        temp_eval.append((tcol, vcol, i, j))

X_A = np.array(X_rows)  # (n_samples, 19)
y_A = np.array(y_rows)  # (n_samples,)

# è®­ç»ƒ A_k ç³»æ•°æ¨¡å‹
A_solver = HuberRegressor(fit_intercept=False, max_iter=5000)
A_solver.fit(X_A, y_A)
A_vec = A_solver.coef_  # é•¿åº¦19ï¼Œå¯¹åº”19ä¸ªåŸºå›¢

# ==== 6. ç”ŸæˆåŸºå‡†å‰å¸ƒæ–¯è‡ªç”±èƒ½é¢„æµ‹ ====
V_pred_baseline = pd.DataFrame(index=df.index, columns=v_cols, dtype=float)

for i in range(len(df)):  # éå†æ‰€æœ‰ç‰©è´¨
    Tb_i = Tb_pred_all[i]  # ç‰©è´¨içš„å‚è€ƒæ¸©åº¦
    HVap_Tb_i = HVap_Tb_all[i]  # ç‰©è´¨içš„å‚è€ƒå‰å¸ƒæ–¯è‡ªç”±èƒ½å€¼

    for j, (tcol, vcol) in enumerate(zip(temp_cols, v_cols)):
        Tj = df.at[i, tcol]  # æ¸©åº¦å€¼

        if np.isnan(Tj):
            V_pred_baseline.at[i, vcol] = np.nan
            continue

        # ç‰¹å¾ï¼š(T - T_ref) Ã— G (ä½¿ç”¨åŸå§‹19ä¸ªåŸºå›¢)
        Xj = (Tj - Tb_i) * G[i]

        # é¢„æµ‹ï¼šV_ref + A_k Ã— (T - T_ref) Ã— G
        V_pred_j = HVap_Tb_i + Xj @ A_vec
        V_pred_baseline.at[i, vcol] = V_pred_j

# ==== 7. æ®‹å·®æœºå™¨å­¦ä¹ æ¨¡å‹ ====
print("è®­ç»ƒæ®‹å·®æœºå™¨å­¦ä¹ æ¨¡å‹...")

# æ„å»ºæ®‹å·®è®­ç»ƒæ•°æ®é›†
residual_features = []
residual_targets = []
sample_info = []  # ä¿å­˜æ ·æœ¬ä¿¡æ¯ç”¨äºè¿½è¸ª

for tcol, vcol in zip(temp_cols, v_cols):
    Tj = df[tcol].to_numpy()
    Vj = df[vcol].to_numpy()
    msk = (~np.isnan(Tj)) & (~np.isnan(Vj))

    for i in np.where(msk)[0]:
        # åŸºç¡€ç‰¹å¾ï¼šåŸºå›¢ç»„æˆ
        base_features = list(G[i])

        # æ¸©åº¦ç›¸å…³ç‰¹å¾
        temp_features = [
            Tj[i],  # ç»å¯¹æ¸©åº¦
            Tj[i] - Tb_pred_all[i],  # ç›¸å¯¹äºå‚è€ƒæ¸©åº¦çš„å·®å€¼
            Tj[i] / Tb_pred_all[i] if Tb_pred_all[i] > 0 else 0,  # ç›¸å¯¹æ¸©åº¦
            np.log(Tj[i]) if Tj[i] > 0 else 0,  # æ¸©åº¦å¯¹æ•°
        ]

        # åŸºå‡†é¢„æµ‹å€¼ä½œä¸ºç‰¹å¾
        baseline_pred = V_pred_baseline.at[i, vcol]
        baseline_features = [baseline_pred]

        # å‚è€ƒå€¼ç‰¹å¾
        ref_features = [
            Tb_pred_all[i],  # å‚è€ƒæ¸©åº¦
            HVap_Tb_all[i],  # å‚è€ƒå‰å¸ƒæ–¯è‡ªç”±èƒ½å€¼
        ]

        # ç»„åˆæ‰€æœ‰ç‰¹å¾
        all_features = base_features + temp_features + baseline_features + ref_features
        residual_features.append(all_features)

        # æ®‹å·®ç›®æ ‡ï¼šå®é™…å€¼ - åŸºå‡†é¢„æµ‹å€¼
        residual = Vj[i] - baseline_pred
        residual_targets.append(residual)

        sample_info.append((i, tcol, vcol))

residual_features = np.array(residual_features)
residual_targets = np.array(residual_targets)

print(f"æ®‹å·®è®­ç»ƒé›†å½¢çŠ¶: {residual_features.shape}")
print(f"æ®‹å·®ç›®æ ‡å½¢çŠ¶: {residual_targets.shape}")

# æ ‡å‡†åŒ–ç‰¹å¾
scaler_residual = StandardScaler()
residual_features_scaled = scaler_residual.fit_transform(residual_features)

# è®­ç»ƒæ®‹å·®æ¨¡å‹ï¼ˆä½¿ç”¨æ¢¯åº¦æå‡å›å½’ï¼‰
residual_model = GradientBoostingRegressor(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42
)

# äº¤å‰éªŒè¯è¯„ä¼°æ®‹å·®æ¨¡å‹
cv_scores = cross_val_score(residual_model, residual_features_scaled, residual_targets,
                            cv=5, scoring='r2')
print(f"æ®‹å·®æ¨¡å‹äº¤å‰éªŒè¯ RÂ²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# è®­ç»ƒæœ€ç»ˆæ®‹å·®æ¨¡å‹
residual_model.fit(residual_features_scaled, residual_targets)

# ==== 8. ç”Ÿæˆæœ€ç»ˆé¢„æµ‹ï¼ˆåŸºå‡† + æ®‹å·®ä¿®æ­£ï¼‰ ====
V_pred_final = pd.DataFrame(index=df.index, columns=v_cols, dtype=float)

for tcol, vcol in zip(temp_cols, v_cols):
    Tj = df[tcol].to_numpy()

    # ä¸ºæ‰€æœ‰æ ·æœ¬æ„å»ºç‰¹å¾
    features_list = []
    valid_indices = []

    for i in range(len(df)):
        if np.isnan(Tj[i]):
            continue

        # æ„å»ºä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç‰¹å¾
        base_features = list(G[i])
        temp_features = [
            Tj[i],
            Tj[i] - Tb_pred_all[i],
            Tj[i] / Tb_pred_all[i] if Tb_pred_all[i] > 0 else 0,
            np.log(Tj[i]) if Tj[i] > 0 else 0,
        ]
        baseline_pred = V_pred_baseline.at[i, vcol]
        baseline_features = [baseline_pred]
        ref_features = [Tb_pred_all[i], HVap_Tb_all[i]]

        all_features = base_features + temp_features + baseline_features + ref_features
        features_list.append(all_features)
        valid_indices.append(i)

    if features_list:
        features_array = np.array(features_list)
        features_scaled = scaler_residual.transform(features_array)

        # é¢„æµ‹æ®‹å·®
        residual_pred = residual_model.predict(features_scaled)

        # æœ€ç»ˆé¢„æµ‹ = åŸºå‡†é¢„æµ‹ + æ®‹å·®ä¿®æ­£
        for idx, residual_val in zip(valid_indices, residual_pred):
            final_pred = V_pred_baseline.at[idx, vcol] + residual_val
            V_pred_final.at[idx, vcol] = final_pred

    # å¯¹äºæ— æ•ˆæ¸©åº¦ç‚¹ï¼Œä¿æŒNaN
    V_pred_final[vcol] = np.where(np.isnan(Tj), np.nan, V_pred_final[vcol])

# ==== 9. è¯„ä¼°æ¨¡å‹æ€§èƒ½ ====
# åŸºå‡†æ¨¡å‹è¯„ä¼°
print("\n=== åŸºå‡†æ¨¡å‹æ€§èƒ½ ===")
y_true_all, y_pred_baseline = [], []
for vcol in v_cols:
    m = (~df[vcol].isna()) & (~V_pred_baseline[vcol].isna())
    if m.any():
        y_true_all.append(df.loc[m, vcol].to_numpy())
        y_pred_baseline.append(V_pred_baseline.loc[m, vcol].to_numpy())

y_true_all = np.concatenate(y_true_all)
y_pred_baseline = np.concatenate(y_pred_baseline)

mse_baseline = mean_squared_error(y_true_all, y_pred_baseline)
r2_baseline = r2_score(y_true_all, y_pred_baseline)
print(f"åŸºå‡†æ¨¡å‹ - MSE: {mse_baseline:.6f}, RÂ²: {r2_baseline:.6f}")

# æœ€ç»ˆæ¨¡å‹è¯„ä¼°
print("\n=== æœ€ç»ˆæ¨¡å‹æ€§èƒ½ï¼ˆåŸºå‡† + æ®‹å·®ä¿®æ­£ï¼‰===")
y_true_all, y_pred_final = [], []
for vcol in v_cols:
    m = (~df[vcol].isna()) & (~V_pred_final[vcol].isna())
    if m.any():
        y_true_all.append(df.loc[m, vcol].to_numpy())
        y_pred_final.append(V_pred_final.loc[m, vcol].to_numpy())

y_true_all = np.concatenate(y_true_all)
y_pred_final = np.concatenate(y_pred_final)

mse_final = mean_squared_error(y_true_all, y_pred_final)
r2_final = r2_score(y_true_all, y_pred_final)
print(f"æœ€ç»ˆæ¨¡å‹ - MSE: {mse_final:.6f}, RÂ²: {r2_final:.6f}")

# æ”¹è¿›ç¨‹åº¦
improvement = r2_final - r2_baseline
print(f"\næ”¹è¿›ç¨‹åº¦: RÂ² æå‡äº† {improvement:.4f} ({improvement / r2_baseline * 100:.2f}%)")

# ==== 10. åˆ†æ¸©åº¦ç‚¹è¯„ä¼° ====
print("\nåˆ†æ¸©åº¦ç‚¹è¯„ä¼°:")
for tcol, vcol in zip(temp_cols, v_cols):
    m = (~df[tcol].isna()) & (~df[vcol].isna()) & (~V_pred_final[vcol].isna())
    if m.any():
        v_true = df.loc[m, vcol].to_numpy()
        v_pred = V_pred_final.loc[m, vcol].to_numpy()
        mse_temp = mean_squared_error(v_true, v_pred)
        r2_temp = r2_score(v_true, v_pred)
        print(f"  {tcol}: MSE = {mse_temp:.6f}, R2 = {r2_temp:.6f}")

# ==== 11. ä¿å­˜ç»“æœ ====
id_col = df.columns[0]  # ç‰©è´¨ID/åç§°æ‰€åœ¨åˆ—
out_path = "gibbs_free_energy_actual_vs_pred_with_residual_correction.xlsx"

rows = []
for idx, _ in df.iterrows():
    ID = df.at[idx, id_col]
    for j, (tcol, vcol) in enumerate(zip(temp_cols, v_cols), start=1):
        T = df.at[idx, tcol]
        V_act = df.at[idx, vcol]
        V_base = V_pred_baseline.at[idx, vcol] if pd.notna(V_pred_baseline.at[idx, vcol]) else np.nan
        V_final = V_pred_final.at[idx, vcol] if pd.notna(V_pred_final.at[idx, vcol]) else np.nan

        # è®¡ç®—è¯¯å·®
        err_base = (V_base - V_act) if (pd.notna(V_base) and pd.notna(V_act)) else np.nan
        err_final = (V_final - V_act) if (pd.notna(V_final) and pd.notna(V_act)) else np.nan
        residual_correction = (V_final - V_base) if (pd.notna(V_final) and pd.notna(V_base)) else np.nan

        rows.append({
            id_col: ID,
            "temp_index": j,
            "temp_col": tcol,
            "T": T,
            "Gibbs_Free_Energy_actual": V_act,
            "Gibbs_Free_Energy_baseline": V_base,
            "Gibbs_Free_Energy_final": V_final,
            "error_baseline": err_base,
            "error_final": err_final,
            "residual_correction": residual_correction,
            "T_ref": Tb_pred_all[idx],
            "Gibbs_Free_Energy_ref": HVap_Tb_all[idx]
        })

long_compare = pd.DataFrame(rows).sort_values([id_col, "temp_index"])

with pd.ExcelWriter(out_path, engine="xlsxwriter") as writer:
    long_compare.to_excel(writer, sheet_name="compare_long", index=False)

print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {out_path}")
# â€”â€” ç®€æ´ç›¸å¯¹è¯¯å·®ç»Ÿè®¡ï¼ˆæœ€ç»ˆï¼‰â€”â€”
relative_error_final = np.abs((y_pred_final - y_true_all) / y_true_all) * 100
within_1pct_final  = np.sum(relative_error_final <= 1)
within_5pct_final  = np.sum(relative_error_final <= 5)
within_10pct_final = np.sum(relative_error_final <= 10)
ard_final = np.mean(relative_error_final)

print("\nğŸ“Š æ€»æ¨¡å‹è¯„ä¼°ï¼ˆåŸºå‡† + æ®‹å·®ä¿®æ­£ï¼‰ï¼š")
print(f"RÂ²  = {r2_final:.4f}")
print(f"MSE = {mse_final:.6f}")
print(f"ARD = {ard_final:.2f}%")
print(f"âœ… è¯¯å·® â‰¤ 1% çš„æ•°æ®ç‚¹æ•°é‡: {within_1pct_final}")
print(f"âœ… è¯¯å·® â‰¤ 5% çš„æ•°æ®ç‚¹æ•°é‡: {within_5pct_final}")
print(f"âœ… è¯¯å·® â‰¤ 10% çš„æ•°æ®ç‚¹æ•°é‡: {within_10pct_final}")
