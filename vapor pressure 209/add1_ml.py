import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import HuberRegressor
from sklearn.ensemble import GradientBoostingRegressor
from scipy.optimize import least_squares

# ========== è¯»å–æ•°æ® ========== #
df = pd.read_excel("vp209.xlsx", sheet_name='Sheet1')

# ç‰¹å¾æå–
MW = df.iloc[:, 4].values.reshape(-1, 1)
Nc = df.iloc[:, 10].values.reshape(-1, 1)
Ncs = df.iloc[:, 9].values.reshape(-1, 1)
Nk = df.iloc[:, 12:31].values  # 19ä¸ªåŸºå›¢
T = df.iloc[:, 31:41].values
P_vp = df.iloc[:, 41:51].values

# ========= æ¸…ç†éæ³•æ ·æœ¬ ========= #
valid_mask = np.isfinite(P_vp) & (P_vp > 0)
valid_mask = valid_mask.all(axis=1)
MW, Nc, Ncs, Nk, T, P_vp = [x[valid_mask] for x in [MW, Nc, Ncs, Nk, T, P_vp]]

# ========= æ„å»º Nk_poly ========= #
poly = PolynomialFeatures(degree=2, include_bias=False)
Nk_poly = poly.fit_transform(Nk)

# ========= Tb æ¨¡å‹ ========= #
Tb0 = 222.543
Tb = df.iloc[:, 5].values[valid_mask]
model_tb = HuberRegressor(max_iter=10000).fit(Nk_poly, np.exp(Tb / Tb0))
Tb_pred = Tb0 * np.log(np.clip(model_tb.predict(Nk_poly), 1e-6, None))

# åªå¯¹æ¯ä¸ªç‰©è´¨çš„ç‰¹å¾è¿›è¡Œä¸€æ¬¡é¢„æµ‹
P_vp_Tb_pred = np.log(np.clip(model_tb.predict(Nk_poly), 1e-6, None))  # ç›´æ¥å¯¹ Nk_poly è¿›è¡Œé¢„æµ‹

# è®¡ç®— TB æ®‹å·®
P_vp_Tb_true = np.log(P_vp[:, 0])  # å¯¹åº”ç¬¬ä¸€åˆ—çš„è’¸æ±½å‹ P_vp_Tb_true
residual_Tb = P_vp_Tb_true - P_vp_Tb_pred  # å¯¹æ•°æ®‹å·®

# ========= Tc æ¨¡å‹ (Gradient Boosting) ========= #
Tc_half = df['ASPEN Half Critical T'].values[valid_mask]
gb_model_tc = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=0)
gb_model_tc.fit(Nk_poly, Tc_half)
Tc_pred = gb_model_tc.predict(Nk_poly)
Tc_pred_full = Tc_pred * 2

# ========= Pc æ¨¡å‹ ========= #
Pc_bar = df.iloc[:, 51].values[valid_mask]
MW_flat = MW.flatten()
Pc_poly = poly.fit_transform(Nk)

def residual_pc(params, X, MW, Pc_true):
    beta = params[:-1]
    beta3 = params[-1]
    y_pred = X @ beta
    x_pred = y_pred + 0.108998
    Pc_pred = 5.9827 + (1 / x_pred) ** 2 + beta3 * np.exp(1 / MW)
    return Pc_pred - Pc_true

params_init_pc = np.zeros(Pc_poly.shape[1] + 1)
result_pc = least_squares(residual_pc, x0=params_init_pc, args=(Pc_poly, MW_flat, Pc_bar), max_nfev=5000)
x_fit = Pc_poly @ result_pc.x[:-1] + 0.108998
Pc_pred = (5.9827 + (1 / x_fit) ** 2 + result_pc.x[-1] * np.exp(1 / MW_flat)) * 1e5  # Pa

# ========= slope Ã— T ç‰¹å¾æ„å»º ========= #
Pb = 101325  # æ ‡å‡†å¤§æ°”å‹ Pa
slope_all = (np.log(Pc_pred) - np.log(Pb)) / (Tc_pred_full - Tb_pred)
slope_all = slope_all.reshape(-1, 1)
slope_T = slope_all.repeat(10, axis=0) * T.flatten().reshape(-1, 1)

# ========= æ„é€ è®­ç»ƒæ•°æ® ========= #
X = np.hstack([
    Nk.repeat(10, axis=0),
    T.flatten().reshape(-1, 1),
    slope_T
])

# ç›´æ¥æŠŠ TB å’Œ TC æ®‹å·®æ·»åŠ ä¸ºæ–°ç‰¹å¾
X_with_residual = np.hstack([X, residual_Tb.reshape(-1, 1)])

y = np.log(P_vp).flatten()

finite_mask = np.isfinite(y) & np.isfinite(X_with_residual).all(axis=1)
X_with_residual, y = X_with_residual[finite_mask], y[finite_mask]
T_valid = T.flatten()[finite_mask]

# ========= æ¨¡å‹è®­ç»ƒ ========= #
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_with_residual, y)

# ========= æ¨¡å‹è¯„ä¼° ========= #
y_pred = model.predict(X_with_residual)
print("\nğŸ“ˆ è’¸æ±½å‹æ¨¡å‹å¯¹ ln(P) æ‹Ÿåˆç»“æœï¼š")
print(f"RÂ² (lnP) = {r2_score(y, y_pred):.6f}")
print(f"MSE (lnP) = {mean_squared_error(y, y_pred):.6f}")

P_true = np.exp(y)
P_pred = np.exp(y_pred)
mse_P = mean_squared_error(P_true, P_pred)
r2_P = r2_score(P_true, P_pred)
ard_P = np.mean_
